<!DOCTYPE html>
<html>
  <head>
    <title>論文メモ：Deep contextualized word representations – きままにNLP – A Technical Blog</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="文献情報
著者: M. Peters et al.
所属: Allen Institute for Artificial Intelligence / Paul G. Allen School of Computer Science &amp; Engineering, University of Washington
出典: NAACL 2018 (https://aclweb.org/anthology/papers/N/N18/N18-1202/)

" />
    <meta property="og:description" content="文献情報
著者: M. Peters et al.
所属: Allen Institute for Artificial Intelligence / Paul G. Allen School of Computer Science &amp; Engineering, University of Washington
出典: NAACL 2018 (https://aclweb.org/anthology/papers/N/N18/N18-1202/)

" />
    
    <meta name="author" content="きままにNLP" />

    
    <meta property="og:title" content="論文メモ：Deep contextualized word representations" />
    <meta property="twitter:title" content="論文メモ：Deep contextualized word representations" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="きままにNLP - A Technical Blog" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->

    <!-- Favicon head tag -->
    <link rel="icon" href="https://pbs.twimg.com/profile_images/1112635060480442368/Ou7bjYFs_400x400.png" type="image/x-icon">

    <!-- Begin Jekyll SEO tag v2.6.0 -->
<title>論文メモ：Deep contextualized word representations | きままにNLP</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="論文メモ：Deep contextualized word representations" />
<meta property="og:locale" content="ja_JP" />
<meta name="description" content="NAACL 2018でベストペーパーアワードに輝いた，Deep contextualized word representations（通称ELMo）の論文メモ書きを共有・紹介します．" />
<meta property="og:description" content="NAACL 2018でベストペーパーアワードに輝いた，Deep contextualized word representations（通称ELMo）の論文メモ書きを共有・紹介します．" />
<link rel="canonical" href="http://localhost:4000/Note-ELMo/" />
<meta property="og:url" content="http://localhost:4000/Note-ELMo/" />
<meta property="og:site_name" content="きままにNLP" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-06T00:00:00+09:00" />
<script type="application/ld+json">
{"description":"NAACL 2018でベストペーパーアワードに輝いた，Deep contextualized word representations（通称ELMo）の論文メモ書きを共有・紹介します．","@type":"BlogPosting","url":"http://localhost:4000/Note-ELMo/","headline":"論文メモ：Deep contextualized word representations","dateModified":"2019-04-06T00:00:00+09:00","datePublished":"2019-04-06T00:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/Note-ELMo/"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <!-- Google Adsense-->
    <!--
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-1838422896597988",
        enable_page_level_ads: true
      });
    </script>
    -->
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="https://pbs.twimg.com/profile_images/1112635060480442368/Ou7bjYFs_400x400.png" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">きままにNLP</a></h1>
            <p class="site-description">A Technical Blog</p>
          </div>

          <nav>
            <a href="/">Blog</a>
            <a href="/about">About</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <style type="text/css">
  #toc ul, ol{
  color: #1e366a;
  padding: 0.1em 0 0.1em 1.0em;
  font-size: 90%
  }
  
  #toc ul li, ol li{
    line-height: 1.5;
    padding: 0.1em 0;
  }

  #box div{
   border-top: solid #1e366a 1px;/*上のボーダー*/
   border-bottom: solid #1e366a 1px; /* 下側の1本線 */
   padding: 0.1em 0;
  }
</style>

<article class="post">
  <h1>論文メモ：Deep contextualized word representations</h1>
  <div id="box">
    <div id="toc">
      <ul class="date">
  <li><a href="#文献情報">文献情報</a></li>
  <li><a href="#どんな論文か">どんな論文か？</a></li>
  <li><a href="#先行研究と比べてどこが凄い">先行研究と比べてどこが凄い？</a></li>
  <li><a href="#技術や手法のキモはどこ">技術や手法のキモはどこ？</a></li>
  <li><a href="#どうやって有効だと検証した">どうやって有効だと検証した？</a></li>
  <li><a href="#議論はある">議論はある？</a></li>
  <li><a href="#論文の詳細メモ">論文の詳細メモ</a>
    <ul>
      <li><a href="#1-順方向言語モデル">1. 順方向言語モデル</a></li>
      <li><a href="#2-逆方向言語モデル">2. 逆方向言語モデル</a></li>
      <li><a href="#3-双方向言語モデル">3. 双方向言語モデル</a></li>
      <li><a href="#4-elmoのパラメータ算出">4. ELMoのパラメータ算出</a></li>
      <li><a href="#5-elmoモデルのnlpタスクへの適用">5. ELMoモデルのNLPタスクへの適用</a></li>
    </ul>
  </li>
  <li><a href="#まとめスライド">まとめスライド</a></li>
  <li><a href="#実装">実装</a></li>
</ul>
    </div>
  </div> 
  <div class="entry">
    <h2 id="文献情報">文献情報</h2>
<p>著者: M. Peters et al.<br />
所属: Allen Institute for Artificial Intelligence / Paul G. Allen School of Computer Science &amp; Engineering, University of Washington<br />
出典: NAACL 2018 <a href="https://aclweb.org/anthology/papers/N/N18/N18-1202/">(https://aclweb.org/anthology/papers/N/N18/N18-1202/)</a></p>

<h2 id="どんな論文か">どんな論文か？</h2>
<p>ELMo = Embeddings from Language Modelsの略であり，言語モデルを活用した文脈に応じた単語分散表現: ELMoを提唱した論文．</p>

<h2 id="先行研究と比べてどこが凄い">先行研究と比べてどこが凄い？</h2>
<p>既存のNLPタスクのモデルの埋め込み層にELMoを追加するだけで，様々なタスクで当時のSoTAを達成した．</p>

<h2 id="技術や手法のキモはどこ">技術や手法のキモはどこ？</h2>
<p>双方向言語モデルを活用し，隠れ層の重みを重み付き線形和で圧縮する点．<br />
→ 従来手法では，単に双方向言語モデルの出力層だけを取ってきていた．</p>

<h2 id="どうやって有効だと検証した">どうやって有効だと検証した？</h2>
<ul>
  <li>各種NLPタスクに適用して，そのスコアにより評価．</li>
</ul>

<h2 id="議論はある">議論はある？</h2>
<ul>
  <li>ELMoの中間層が捉えている特徴素について<br />
→ 著者らによると，ELMoの中間層は低層であればあるほど，<strong>文法的(syntactic)</strong>な情報を含み，高層の方が，<strong>意味的(semantic)</strong>な情報を含むとのこと．<br />
→ 基本的にどのタスクもsyntacticな情報を好む傾向にあるらしい．</li>
</ul>

<h2 id="論文の詳細メモ">論文の詳細メモ</h2>
<h3 id="1-順方向言語モデル">1. 順方向言語モデル</h3>
<p>トークン数: $N$の単語列: $(t_1, t_2, \dots, t_N)$が与えられたとき，順方向の言語モデルは，単語: $t_k$と単語列: $(t_1, \dots, t_{k-1})$の条件付き確率をモデリングすることで，次のように表される.順方向の言語モデルでは，次の単語: $t_{k+1}$を予測することを目的とする．</p>

<script type="math/tex; mode=display">p(t_1, t_2, ..., t_N) = \prod_{k=1}^N p(t_k | t_1, t_2, \dots, t_{k-1})</script>

<ul>
  <li>
    <p>最近の言語モデルでは，文脈非依存な単語表現: $\mathbf{x}_k^{LM}$を単語埋め込みやcharacter-based CNNにより算出してから，L層のLSTMに入力することが多い．</p>
  </li>
  <li>
    <p>単語列の位置: $k$において，各LSTM層は，文脈依存な単語表現: $\overrightarrow{\mathbf{h}}_{k, j}^{LM}$（ただし，$1 \leq j \leq L$）を出力する．</p>
  </li>
  <li>
    <p><script type="math/tex">\overrightarrow{\mathbf{h}}_{k, L}^{LM}</script>は，次の単語: $t_{k+1}$をsoftmax層で予測するのに用いられる．</p>
  </li>
</ul>

<h3 id="2-逆方向言語モデル">2. 逆方向言語モデル</h3>
<p>逆方向言語モデルは順方向言語モデルと類似しているが，単語列を逆に処理していく点で異なる．なお，逆方向の言語モデルでは，未来の単語列から一つ前の単語: $t_{k-1}$を予測することを目的とする．つまり，逆方向の言語モデルは次のように定義される．</p>

<script type="math/tex; mode=display">p(t_1, t_2, ..., t_N) = \prod_{k=1}^N p(t_k | t_{k+1}, t_{k+2}, \dots, t_N)</script>

<ul>
  <li>最終的には，<script type="math/tex">\overleftarrow{\mathbf{h}}_{k, L}^{LM}</script> を求めることで，$t_{k-1}$ をsoftmax層で予測する．</li>
</ul>

<h3 id="3-双方向言語モデル">3. 双方向言語モデル</h3>
<ul>
  <li>
    <p>通常言語モデルは対数尤度を最大化することで最適化を行う．<br />
→ 文脈中で出現してほしい単語の確率を最大化するため．</p>
  </li>
  <li>
    <p>ELMoで使われる双方向言語モデルは次の式の対数尤度を最大化することで学習する．<br />
→ 文脈非依存な単語表現とソフトマックス層のパラメータ: $\Theta_{x}, \Theta_s$は共有．LSTMのパラメータについてのみ独立．<br />
→ 従来手法ではパラメータはすべて独立</p>
  </li>
</ul>

<script type="math/tex; mode=display">\sum_{k=1}^{N}(\log p(t_k | t_1, t_2, \dots, t_{k-1}; \Theta_x, \overrightarrow{\Theta}_{LSTM}, \Theta_s) \\+ \log p(t_k | t_{k+1}, t_{k+2}, \dots, t_N; \Theta_x, \overleftarrow{\Theta}_{LSTM}, \Theta_s))</script>

<h3 id="4-elmoのパラメータ算出">4. ELMoのパラメータ算出</h3>
<ul>
  <li>全てのトークン: $t_k$はL層の双方向言語モデルに対し，2L+1個の特徴量を持つ．<br />
→ 文脈依存しない単語ベクトル: 1個<br />
→ 文脈依存のする順方向と逆方向のLSTM: 2L個</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation*}
\begin{split}
R_k &= \{x_k^{LM}, \overrightarrow{\mathbf{h}}_{k, j}^{LM}, \overleftarrow{\mathbf{h}}_{k, j}^{LM} | j = 1, \dots, L \}\\
    &= \{\mathbf{h}_{k, j}^{LM} | j=0, \dots, L \}
\end{split}
\end{equation*} %]]></script>

<ul>
  <li>ELMoを他のタスクに応用するには，Rのすべての要素を一つのベクトル表現に変換する．<br />
→ 簡単な例だと，一番上の層だけを取ってくるものがある．CoVeやTagLMはこれを採用している．</li>
</ul>

<script type="math/tex; mode=display">\mathbf{ELMo}_k^{task} = E(R_k; \Theta_e) = \mathbf{h}_{k, L}^{LM}</script>

<div style="text-align: center">ただし，$\mathbf{h}_{k, L}^{LM} = \left[\overrightarrow{\mathbf{h}}_{k, j}^{LM}; \overleftarrow{\mathbf{h}}_{k, j}^{LM}\right]$</div>

<ul>
  <li>ELMoではより便宜をはかり，重み付き線形和の形で変換する．<br />
→ $s_j^{task}$はsoftmaxで正規化された重み<br />
→ $\gamma^{task}$はELMoのベクトル全体をスケーリングするため．チューニングの最適化の観点から必要．</li>
</ul>

<script type="math/tex; mode=display">\mathbf{ELMo}_k^{task} = \gamma^{task} \sum_{j=0}^L s_j^{task}\mathbf{h}_{k, j}^{LM}</script>

<h3 id="5-elmoモデルのnlpタスクへの適用">5. ELMoモデルのNLPタスクへの適用</h3>
<p>単に$\mathbf{ELMo}_k^{task}$を入力の埋め込みベクトルとconcatすれば良い．</p>

<h2 id="まとめスライド">まとめスライド</h2>
<div style="text-align: center"><iframe src="//www.slideshare.net/slideshow/embed_code/key/hvw0gfJhsc8aWL" width="510" height="420" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen=""> </iframe></div>
<div style="margin-bottom:5px"></div>

<h2 id="実装">実装</h2>
<p>試しにKerasでELMo + BiLSTMを使ってIMDBの分類を行ったので，GitHubにあげました．<br />
→ <a href="https://github.com/gucci-j/elmo-imdb">https://github.com/gucci-j/elmo-imdb</a></p>


  </div>

  <div class="date">
    Written on April  6, 2019
  </div>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- フッター1 -->
  <ins class="adsbygoogle"
      style="display:block"
      data-ad-client="ca-pub-1838422896597988"
      data-ad-slot="4583840862"
      data-ad-format="auto"
      data-full-width-responsive="true"></ins>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
  </script>

  
  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          
<a href="mailto:gucci.research@gmail.com"><i class="svg-icon email"></i></a>


<a href="https://github.com/gucci-j"><i class="svg-icon github"></i></a>



<a href="/feed.xml"><i class="svg-icon rss"></i></a>
<a href="https://www.twitter.com/_gucciiiii"><i class="svg-icon twitter"></i></a>


<br />
          <a href="/">Home</a> | <a href="/privacy">Privacy Policy</a><br />
          <div style="font-size: 8pt">© 2019 Atsuki Yamaguchi.</div>
        </footer>
      </div>
    </div>

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-137498199-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/Note-ELMo/',
		  'title': '論文メモ：Deep contextualized word representations'
		});
	</script>
	<!-- End Google Analytics -->


  </body>
</html>
