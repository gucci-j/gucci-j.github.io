<!DOCTYPE html>
<html>
  <head>
    <title>論文メモ：Frustratingly Short Attention Spans in Neural Language Modeling – きままにNLP – A Technical Blog about NLP and ML</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="文献情報

" />
    <meta property="og:description" content="文献情報

" />
    
    <meta name="author" content="きままにNLP" />

    
    <meta property="og:title" content="論文メモ：Frustratingly Short Attention Spans in Neural Language Modeling" />
    <meta property="twitter:title" content="論文メモ：Frustratingly Short Attention Spans in Neural Language Modeling" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="きままにNLP - A Technical Blog about NLP and ML" href="/feed.xml" />
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/earlyaccess/notosansjapanese.css" rel="stylesheet">
    
    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->

    <!-- Favicon head tag -->
    <link rel="icon" href="https://pbs.twimg.com/profile_images/1112635060480442368/Ou7bjYFs_400x400.png" type="image/x-icon">

    <!-- Begin Jekyll SEO tag v2.6.0 -->
<title>論文メモ：Frustratingly Short Attention Spans in Neural Language Modeling | きままにNLP</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="論文メモ：Frustratingly Short Attention Spans in Neural Language Modeling" />
<meta property="og:locale" content="ja_JP" />
<meta name="description" content="Key-Value型のAttentionを活用した言語モデルに関する論文：「Frustratingly Short Attention Spans in Neural Language Modeling」の論文のメモ書きを共有・紹介します．" />
<meta property="og:description" content="Key-Value型のAttentionを活用した言語モデルに関する論文：「Frustratingly Short Attention Spans in Neural Language Modeling」の論文のメモ書きを共有・紹介します．" />
<link rel="canonical" href="http://localhost:4000/Note-Frustratingly/" />
<meta property="og:url" content="http://localhost:4000/Note-Frustratingly/" />
<meta property="og:site_name" content="きままにNLP" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-04-01T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="論文メモ：Frustratingly Short Attention Spans in Neural Language Modeling" />
<meta name="twitter:site" content="@_gucciiiii" />
<script type="application/ld+json">
{"description":"Key-Value型のAttentionを活用した言語モデルに関する論文：「Frustratingly Short Attention Spans in Neural Language Modeling」の論文のメモ書きを共有・紹介します．","@type":"BlogPosting","url":"http://localhost:4000/Note-Frustratingly/","headline":"論文メモ：Frustratingly Short Attention Spans in Neural Language Modeling","dateModified":"2019-04-01T00:00:00+09:00","datePublished":"2019-04-01T00:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/Note-Frustratingly/"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <!-- Google Adsense-->
    <!--
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-1838422896597988",
        enable_page_level_ads: true
      });
    </script>
    -->
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="../resources/logo/logo.jpg" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">きままにNLP</a></h1>
            <p class="site-description">A Technical Blog about NLP and ML</p>
          </div>

          <nav>
            <a href="/">Blog</a>
            <a href="/about">About</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>論文メモ：Frustratingly Short Attention Spans in Neural Language Modeling</h1>

  <div class="post-tag">
    




<ul>
  <i class="fas fa-tag" style="padding: 0 2px 0 0;"></i>
  
    <li>
      <a href="/sitemap#論文メモ">
        論文メモ
      </a>
    </li>
  
</ul>
  </div>

  <div class="toc">
    <input type="checkbox" id="toc_lb" class="on-off" />
    <label for="toc_lb">目次</label>
    <ul>
  <li><a href="#文献情報">文献情報</a></li>
  <li><a href="#この論文の主張">この論文の主張</a></li>
  <li><a href="#背景関連研究">背景・関連研究</a></li>
  <li><a href="#従来手法-attention-for-neural-language-modeling">従来手法 (Attention for Neural Language Modeling)</a></li>
  <li><a href="#提案手法">提案手法</a>
    <ul>
      <li><a href="#key-value-attention">Key-value attention</a></li>
      <li><a href="#key-value-predict-attention">Key-value-predict attention</a></li>
    </ul>
  </li>
  <li><a href="#実験結果">実験結果</a>
    <ul>
      <li><a href="#評価指標">評価指標</a>
        <ul>
          <li><a href="#パープレキシティ">パープレキシティ</a></li>
        </ul>
      </li>
      <li><a href="#評価結果">評価結果</a></li>
    </ul>
  </li>
  <li><a href="#議論">議論</a></li>
  <li><a href="#次に読むべき論文は">次に読むべき論文は？</a></li>
</ul>
  </div>

  <div class="entry">
    <h2 id="文献情報">文献情報</h2>

<p>著者: M. Daniluk et al.<br />
所属: University College London<br />
出典: ICLR 2017 <a href="https://arxiv.org/abs/1702.04521">(https://arxiv.org/abs/1702.04521)</a></p>

<h2 id="この論文の主張">この論文の主張</h2>

<ol>
  <li>ニューラル言語モデルのためのkey-valueに基づくAttentionを提案</li>
  <li>さらにそれを改良したkey-value-predictに基づくAttentionの提案</li>
  <li>従来のMemory-augumented言語モデルよりも，パープレキシティが小さくなった</li>
</ol>

<h2 id="背景関連研究">背景・関連研究</h2>

<p>言語モデルは次に出現する単語を予測する能力を持っている．古典的なNグラムに基づく言語モデルは短文内での単語間の依存関係を捉えることができる．<br />
一方で，ニューラル言語モデルは，より広範囲な単語間の依存関係を捉えることができる．</p>

<p>近年のニューラル言語モデルはAttentionに基づくものが多く，より直接的に単語間の関係性を捉えられるようになってきている．</p>

<p>Attentionを言語モデルに取り入れるには，モデル中の出力ベクトルが以下の複数の役割を同時にこなさなければならない．</p>
<ol>
  <li>次の単語を予測するためにある分布をエンコードする役割</li>
  <li>attentionベクトルを計算するためのベクトルとして振る舞う役割</li>
  <li>attentionにおいて文脈ベクトルを求めるために使われる役割-&gt;次のトークンを予測する際に文脈を考慮するのを助ける役割</li>
</ol>

<div style="text-align: center;">
    <img src="/resources/2019-04-01/k-v-p_attention.png" alt="k-v-p attention" style="width: auto;" />
</div>

<p>一つの出力ベクトルに複数の役割を課すことは，モデルを複雑化させる要因となり，学習を難しくすると考えられる．<br />
したがって，本研究では，上記の三つの役割を別々のベクトルに割り当てることで，モデルを簡単化させることを目指す．<br />
具体的には，各時刻において出力されるベクトルを3つにするということである．<br />
論文内ではこれを，key-value-predictベクトルと名付けており，Attentionを含めて，key-value-predict Attentionと名付けている．</p>

<h2 id="従来手法-attention-for-neural-language-modeling">従来手法 (Attention for Neural Language Modeling)</h2>

<p>(1) <strong>時刻: $t$において，$L$個の出力ベクトルを記憶領域として取る</strong></p>

<script type="math/tex; mode=display">\begin{equation*}
    Y_t = [{\bf h}_{t-L}, \dots, {\bf h}_{t-1}] \in \mathbb{R}^{k \times L}
\end{equation*}</script>

<p>ただし，<script type="math/tex">k</script>はLSTMの出力ベクトルの次元を指し，$h_t \in \mathbb{R}^k$は時刻: $t$における出力ベクトルを意味する．</p>

<p>→ L個に限るのは実用的な問題から: Lはハイパーパラメータ</p>

<p>(2) <strong>Attentionの重み: $\alpha_t$を計算する</strong></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation*}
\begin{split}
\alpha_t &= {\rm softmax}(w^TM_t) \\
M_t &= {\rm tanh}(W^YY_t + (W^hh_t)1^T)
\end{split}
\end{equation*} %]]></script>

<p>ただし， $W^Y, W^h \in \mathbb{R}^{k \times k}, w \in \mathbb{R}^k$は学習パラメータである．また，$1 \in \mathbb{R}^L$である．</p>

<p>→ ここでは時刻: $t$の出力ベクトル: $h_t$とそれ以前のL個の出力ベクトル: $Y_t$がどの程度関係しあっているかを求めている．<br />
つまりL個のトークンの各重要度を算出している．</p>

<p>(3) <strong>Attentionベクトルを生成する</strong><br />
上記で算出したAttentionの重みを基に，Attentionベクトルを生成する．</p>

<script type="math/tex; mode=display">r_t = Y_t\alpha^T</script>

<p>(4) <strong>Attentionベクトルと元の出力ベクトルを結合する</strong><br />
Concatではなく，以下の式に基づいて非線形に結合する．</p>

<script type="math/tex; mode=display">h_t^* = {\rm tanh}(W^rr_t + W^xh_t)\\</script>

<p>ただし，$W^r, W^x \in \mathbb{R}^{k \times k}$は学習パラメータである．</p>

<p>(5) <strong>出力ベクトルを求める</strong></p>

<script type="math/tex; mode=display">y_t = {\rm softmax}(W^*h_t^* + b)</script>

<p>ただし，$W^* \in \mathbb{R}^{|V| \times k}$であり，$b \in \mathbb{R}^{|V|}$である．<br />
ともに学習パラメータである．</p>

<div style="text-align: center;">
    <img src="/resources/2019-04-01/attention.png" alt="ふつうのAttentionモデル" style="width: auto;" />
</div>

<h2 id="提案手法">提案手法</h2>
<h3 id="key-value-attention">Key-value attention</h3>
<p>key-value Attentionでは，出力ベクトルをkeyとvalueに分割する．<br />
具体的には，時刻: $t$における出力ベクトルを以下のように定義し直す．</p>

<script type="math/tex; mode=display">h_t = [k_t, v_t] \in \mathbb{R}^{2k}</script>

<p>また，　$h_t$が関与する式を書き直すと，以下のようになる．</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{split}
M_t &= {\rm tanh}(W^Y[k_{t-L}, \dots, k_{t-1}] + (W^hk_t)1^T) \\
r_t &= [v_{t-L}, \dots, v_{t-1}]\alpha^T \\
h_t^* &= {\rm tanh}(W^rr_t + W^xv_t)
\end{split} %]]></script>

<p>なお，上記以外の式は前章と同じである．</p>

<p>→ ここで，$k$は検索キーとしての役割を果たしており，$v$はその中身のデータを表していると考えるとわかりやすいかもしれない．</p>

<div style="text-align: center;">
    <img src="/resources/2019-04-01/k-v_attention.png" alt="key-value attention" style="width: auto;" />
</div>

<h3 id="key-value-predict-attention">Key-value-predict attention</h3>
<p>key-value attentionにおいても，valueが複数回使われていることがわかる．<br />
そこで，valueをさらに分割し，key-value-predict型のAttentionを考案した．</p>

<p>→ keyはattentionの重みを計算するのにのみ用いられ，valueは文脈表現をエンコードするのに使われ，predictは次のトークンの分布をエンコードするのに用いられる．<br />
→ 完全分業制が達成されていることがわかる．</p>

<script type="math/tex; mode=display">h_t = [k_t, v_t, p_t] \in \mathbb{R}^{3k}　\\
h_t^* = {\rm tanh}(W^rr_t + W^xp_t)</script>

<div style="text-align: center;">
    <img src="/resources/2019-04-01/k-v-p_attention.png" alt="k-v-p attention" style="width: auto;" />
</div>

<h2 id="実験結果">実験結果</h2>
<h3 id="評価指標">評価指標</h3>
<h4 id="パープレキシティ">パープレキシティ</h4>
<p>参考: <a href="http://www.jnlp.org/lab/graduates/okada/nlp/term/entropy">http://www.jnlp.org/lab/graduates/okada/nlp/term/entropy</a></p>

<p>パープレキシティ(perplexity)とは，言語モデルにおいてモデルの複雑性を評価するのに使われる指標である． <br />
パープレキシティは2のクロスエントロピー乗で定義され，一般に小さいほど良いモデルであるとされる．</p>

<script type="math/tex; mode=display">Perplexity = 2^{-\frac{1}{N}\sum_i^{N}\log P(w_i)}</script>

<p>→ ここでの$P(w_i)$は言語モデルの単語出現確率を表している．<br />
→ パープレキシティは単語の分岐数を意味しており，ある単語に対してそれに続く単語の平均候補数も意味している．</p>

<p>つまり，複雑なモデルであるほど，平均候補数が増加するため，パープレキシティは大きくなるといえる．</p>

<h3 id="評価結果">評価結果</h3>
<p>提案手法： key-Value-Predictのパープレキシティが有意に小さいことがわかる．</p>

<div style="text-align: center;">
    <img src="/resources/2019-04-01/result1.png" alt="実験結果1" style="width: auto;" />
</div>

<div style="text-align: center;">
    <img src="/resources/2019-04-01/result2.png" alt="実験結果2" style="width: auto;" />
</div>

<p>(a)の図から，提案手法: key-value-predict attentionのおかげで，広範囲な文脈を考慮出来るようになっていることがわかる．<br />
(b)の図から，より広範な文脈を考慮したとしても，パープレキシティが大幅に改善することは期待できないということが読み取れる．</p>

<div style="text-align: center;">
    <img src="/resources/2019-04-01/weight.png" alt="注意機構の重み" style="width: auto;" />
</div>

<h2 id="議論">議論</h2>
<p>Attentionを用いた言語モデルによって，従来手法よりもパープレキシティを改善することができた．<br />
しかし，あまりにも長い文脈を考慮することは，パープレキシティの改善には得策ではないこともわかった．</p>

<p>Future workとしては局所的な文脈の内容を考慮しないで，その背景にあるより大域的に関係する文脈を考慮できるような手法を考えることが挙げられる．</p>

<h2 id="次に読むべき論文は">次に読むべき論文は？</h2>
<p>Memory networksとか？<br />
→ key-valueの概念を初めて導入したらしい．</p>

  </div>

  <div class="date">
    Written on April  1, 2019
  </div>

  <style type="text/css">
    @font-face {
	font-family: 'icomoon';
	src:url('/resources/fonts/icomoon.eot?ookgoz');
	src:url('/resources/fonts/icomoon.eot?ookgoz#iefix') format('embedded-opentype'),
		url('/resources/fonts/icomoon.ttf?ookgoz') format('truetype'),
		url('/resources/fonts/icomoon.woff?ookgoz') format('woff'),
		url('/resources/fonts/icomoon.svg?ookgoz#icomoon') format('svg');
		font-weight: normal;
		font-style: normal;
    }

    [class^="icon-"], [class*=" icon-"] {
        /* use !important to prevent issues with browser extensions that change fonts */
        font-family: 'icomoon' !important;
        speak: none;
        font-style: normal;
        font-weight: normal;
        font-variant: normal;
        text-transform: none;
        line-height: inherit;
        
        /* Better Font Rendering =========== */
        -webkit-font-smoothing: antialiased;
        -moz-osx-font-smoothing: grayscale;
    }

    .icon-line:before        {content: "\e90a";}
    .icon-pocket:before      {content: "\e902";}
    .icon-twitter:before     {content: "\ea96";}
    .icon-facebook:before    {content: "\ea90";}
    .icon-hatebu:before      {content: "\e903";}

    /*ソーシャルリストデザイン2*/
    .shareList2 {
        list-style:none;
        display: flex;
        flex-wrap:wrap;
        width:100%;
        margin:-5px 0 0 -5px;
        padding:0;
    }
    .shareList2__item {
        height:30px;
        line-height:30px;
        width:30px;
        margin:5px 0 0 5px;
        text-align:center;
    }
    .shareList2__link {
        display:block;
        color:#ffffff;
        text-decoration: none;
        border-radius: 5px;
    }
    .shareList2__link::before{
        font-size:16px;
        display:block;
        transition: ease-in-out .2s;
        border-radius: 5px;
    }
    .shareList2__link:hover::before{
        background:#ffffff;
        transform: scale(1.2);
        box-shadow:1px 1px 4px 0px rgba(0,0,0,0.15);
    }

    .shareList2__link.icon-twitter{background:#55acee;}
    .shareList2__link.icon-twitter:hover::before{color:#55acee;}

    .shareList2__link.icon-facebook{background:#3B5998;}
    .shareList2__link.icon-facebook:hover::before{color:#3B5998;}

    .shareList2__link.icon-hatebu{background:#008FDE;}
    .shareList2__link.icon-hatebu:hover::before{color:#008FDE;}

    .shareList2__link.icon-pocket{background:#EB4654;}
    .shareList2__link.icon-pocket:hover::before{color:#EB4654;}

    .shareList2__link.icon-line{background:#1dcd00;}
    .shareList2__link.icon-line:hover::before{color:#1dcd00;}
</style>

<ul class="shareList2" style="margin-top: 5px;">
    <li class="shareList2__item"><a class="shareList2__link icon-twitter" href="https://twitter.com/intent/tweet?text=%E8%AB%96%E6%96%87%E3%83%A1%E3%83%A2:Frustratingly%20Short%20Attention%20Spans%20in%20Neural%20Language%20Modeling+–+%E3%81%8D%E3%81%BE%E3%81%BE%E3%81%ABNLP&amp;url=http%3A%2F%2Flocalhost%3A4000%2FNote-Frustratingly%2F" target="_blank" title="Twitter"></a></li>
    <li class="shareList2__item"><a class="shareList2__link icon-facebook" href="https://www.facebook.com/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2FNote-Frustratingly%2F" target="_blank" title="Facebook"></a></li>
    <li class="shareList2__item"><a class="shareList2__link icon-hatebu" href="https://b.hatena.ne.jp/add?mode=confirm&amp;url=http%3A%2F%2Flocalhost%3A4000%2FNote-Frustratingly%2F" target="_blank" title="はてなブックマーク"></a></li>
    <li class="shareList2__item"><a class="shareList2__link icon-pocket" href="//getpocket.com/edit?url=http%3A%2F%2Flocalhost%3A4000&title=%E8%AB%96%E6%96%87%E3%83%A1%E3%83%A2:Frustratingly%20Short%20Attention%20Spans%20in%20Neural%20Language%20Modeling+–+%E3%81%8D%E3%81%BE%E3%81%BE%E3%81%ABNLP" target="_blank" title="Pocket"></a></li>
    <li class="shareList2__item"><a class="shareList2__link icon-line" href="http://line.me/R/msg/text/?%E8%AB%96%E6%96%87%E3%83%A1%E3%83%A2:Frustratingly%20Short%20Attention%20Spans%20in%20Neural%20Language%20Modeling+–+%E3%81%8D%E3%81%BE%E3%81%BE%E3%81%ABNLP%0Ahttp%3A%2F%2Flocalhost%3A4000%2FNote-Frustratingly%2F" target="_blank" title="LINE"></a></li>
</ul>

  <br />

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <!-- フッター1 -->
  <ins class="adsbygoogle"
      style="display:block"
      data-ad-client="ca-pub-1838422896597988"
      data-ad-slot="4583840862"
      data-ad-format="auto"
      data-full-width-responsive="true"></ins>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
  </script>

  
  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          
<a href="/contact/"><i class="svg-icon email"></i></a>


<a href="https://github.com/gucci-j"><i class="svg-icon github"></i></a>



<a href="/feed.xml"><i class="svg-icon rss"></i></a>
<a href="https://www.twitter.com/_gucciiiii"><i class="svg-icon twitter"></i></a>


<br />
          <a href="/">Home</a> 
          | <a href="/sitemap">Sitemap</a> 
          | <a href="/privacy">Privacy Policy</a><br />
          <div style="font-size: 8pt">© 2019 Atsuki Yamaguchi.</div>
        </footer>
      </div>
    </div>

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-137498199-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/Note-Frustratingly/',
		  'title': '論文メモ：Frustratingly Short Attention Spans in Neural Language Modeling'
		});
	</script>
	<!-- End Google Analytics -->


  </body>
</html>
